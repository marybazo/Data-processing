{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install translator for catalog\n",
    "%pip install googletrans==4.0.0-rc1\n",
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from googletrans import Translator\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "root = 'archive/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Description\n",
    "### <a href='https://www.kaggle.com/datasets/svizor/retail-sales-forecasting-data'> Retail sales forcastiong data</a></br>\n",
    "This dataset contains sales information from four stores of one of the retailers over 25 months. </br>\n",
    "Participants are expected to use these files to develop models that can predict customer demand. </br>\n",
    "Additionally, the dataset includes a holdout sample with sales data for a 1-month period for which </br>\n",
    "forecasts should be provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What will be done with data\n",
    "1.\tCreate English names for products. (long process, data store in final_catalog.csv)\n",
    "2.\tCreate synthetic clients, employees, and orders.\n",
    "3.\tRead online sales data and add all dimensions to it.\n",
    "4.\tRead offline data and reduce it from 7.5 million to 1.5 million records.\n",
    "5.\tAdd all necessary dimensions to offline sales.\n",
    "6. Add cost column to bouth datasets. \n",
    "From price history \n",
    "a) by data, item, store; \n",
    "b) max value on item \n",
    "c) fillna with price_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Reading down in code !\n",
    "\n",
    "# sales  = pd.read_csv(root + 'sales.csv', index_col=0)\n",
    "# online  = pd.read_csv(root + 'online.csv', index_col=0)\n",
    "\n",
    "# catalog = pd.read_csv(root + 'catalog.csv', index_col=0)\n",
    "# stores  = pd.read_csv(root + 'stores.csv', index_col=0)\n",
    "\n",
    "# price_history  = pd.read_csv(root + 'price_history.csv', index_col=0)\n",
    "\n",
    "# actual_matrix  = pd.read_csv(root + 'actual_matrix.csv', index_col=0)\n",
    "# discounts_history  = pd.read_csv(root + 'discounts_history.csv', index_col=0)\n",
    "# markdowns  = pd.read_csv(root + 'markdowns.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## catalog.csv\n",
    "Purpose: Product catalog with characteristics.</br>\n",
    "Columns:</br>\n",
    "item_id: A unique identifier for each product</br>\n",
    "dept_name: Product department (hierarchy level)</br>\n",
    "class_name: Product class (hierarchy level)</br>\n",
    "subclass_name: Product subclass (hierarchy level)</br>\n",
    "item_type: Product type</br>\n",
    "weight_volume: Volumetric weight</br>\n",
    "weight_netto: Net weight</br>\n",
    "fatness: Fat content</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pd.read_csv(root + 'catalog.csv', index_col=0)\n",
    "catalog.head(3) # TRANSLATE dept_name class_name subclass_name item_type into ENGLISH, and add to catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process batches to translate\n",
    "def batch_translate(items, batch_size, pause_seconds):\n",
    "    # create df for translation\n",
    "    translator = Translator()\n",
    "    names_translated = pd.DataFrame(columns=['name_ru', 'name_en'])\n",
    "\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        batch = items[i:i+batch_size]\n",
    "        \n",
    "        # Translate each item in the batch\n",
    "        for item in batch:\n",
    "            try:\n",
    "                translation = translator.translate(item, src='ru', dest='en').text\n",
    "                names_translated.loc[len(names_translated)] = [item, translation.capitalize()]\n",
    "            except Exception as e:\n",
    "                names_translated.loc[len(names_translated)] = [item, 'Error']\n",
    "        \n",
    "        # Pause between batches\n",
    "        time.sleep(pause_seconds)\n",
    "    \n",
    "    return names_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values\n",
    "dept_name = catalog['dept_name'].unique()\n",
    "class_name = catalog['class_name'].unique()\n",
    "subclass_name = catalog['subclass_name'].unique()\n",
    "item_type = catalog['item_type'].unique()\n",
    "\n",
    "# Translate in batches of 100 with a 2-second pause between batches\n",
    "batch_size = 100\n",
    "pause_seconds = 2\n",
    "\n",
    "print('dept_name', len(dept_name))\n",
    "print('class_name', len(class_name))\n",
    "print('subclass_name', len(subclass_name))\n",
    "print('item_type', len(item_type))\n",
    "print('item_id', len(catalog['item_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Department name\n",
    "dept_name_translated = batch_translate(dept_name, batch_size, pause_seconds)\n",
    "\n",
    "# ! Manual substitute ! Department\n",
    "# data = dept_name_translated['name_en']\n",
    "# data[data.duplicated(keep=False)].unique()\n",
    "\n",
    "dept_name_translated[dept_name_translated['name_en'] == 'Cakes']\n",
    "dept_name_translated.iloc[10].name_en = 'Cupcakes'\n",
    "\n",
    "# 196\n",
    "len(dept_name_translated['name_en'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class name   \n",
    "class_name_translated = batch_translate(class_name, batch_size, pause_seconds)\n",
    "\n",
    "# ! Manual substitute ! Class\n",
    "data = class_name_translated['name_en']\n",
    "\n",
    "# data[data.duplicated(keep=False)].unique() # list of duplicates\n",
    "# ['Paste', 'Wheat', 'Green', 'Cottage cheese', 'Domestic', 'Salmon',\n",
    "#        'Honey', 'Black', 'Weight', 'Dumplings', 'Cakes', 'Puff', 'Bread',\n",
    "#        'Import', 'Crackers', 'Other', 'Soy sauces']\n",
    "\n",
    "class_name_translated[class_name_translated['name_en'] == 'Paste']\n",
    "class_name_translated.iloc[600].name_en = 'Pasta'\n",
    "\n",
    "class_name_translated[class_name_translated['name_en'] == 'Green']\n",
    "class_name_translated.iloc[32].name_en = 'Greens'\n",
    "\n",
    "class_name_translated[class_name_translated['name_en'] == 'Dumplings']\n",
    "class_name_translated.iloc[285].name_en = 'Varenyki'\n",
    "\n",
    "class_name_translated[class_name_translated['name_en'] == 'Cakes']\n",
    "class_name_translated.iloc[205].name_en = 'Cupcakes'\n",
    "\n",
    "# Change duplication\n",
    "len(class_name_translated['name_en'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix error translation\n",
    "error_value = class_name_translated[class_name_translated['name_en'] == 'Error']['name_ru']\n",
    "try_trans = batch_translate(error_value, batch_size, pause_seconds)\n",
    "try_trans[try_trans['name_en'] == 'Error']\n",
    "\n",
    "name_mapping = try_trans.set_index('name_ru')['name_en'].to_dict()\n",
    "\n",
    "# Replace \"Error\" with the correct values from df2, and keep the original for unmatched keys\n",
    "class_name_translated['name_en'] = class_name_translated.apply(\n",
    "    lambda row: name_mapping.get(row['name_ru'], row['name_en']) if row['name_en'] == 'Error' else row['name_en'], axis=1\n",
    ")\n",
    "\n",
    "# 599\n",
    "len(class_name_translated['name_en'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass name\n",
    "subclass_name_translated = batch_translate(subclass_name, batch_size, pause_seconds)\n",
    "\n",
    "# ! Manual substitute ! Subclass\n",
    "# data = subclass_name_translated['name_en']\n",
    "# data[data.duplicated(keep=False)].unique() # list of duplicates\n",
    "# ['White', 'Piece', 'Wheat', 'Red', 'Green', 'Cakes', 'Potato',\n",
    "#        'Traditional', 'Salmon', 'Weight', 'Own production', 'Jam',\n",
    "#        'Sweet', 'Corn', 'Other accessories', 'Waffle cakes', 'Plates',\n",
    "#        'Crackers', 'Breakfast', 'Cream', 'Lamps', 'Vegetables',\n",
    "#        'Croissants', 'Bread', 'Pear', 'Yeast', 'Paste', 'Other', 'Pasta',\n",
    "#        'Business lunch', 'Protein']\n",
    "\n",
    "subclass_name_translated[subclass_name_translated['name_en'] == 'Green']\n",
    "subclass_name_translated.iloc[32].name_en = 'Greens'\n",
    "\n",
    "subclass_name_translated[subclass_name_translated['name_en'] == 'Cakes']\n",
    "subclass_name_translated.iloc[124].name_en = 'Cupcakes'\n",
    "\n",
    "subclass_name_translated[subclass_name_translated['name_en'] == 'Paste']\n",
    "subclass_name_translated.iloc[432].name_en = 'Pastille'\n",
    "subclass_name_translated.iloc[791].name_en = 'Pasta'\n",
    "\n",
    "len(subclass_name_translated['name_en'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix error translation\n",
    "error_value = subclass_name_translated[subclass_name_translated['name_en'] == 'Error']['name_ru']\n",
    "try_trans = batch_translate(error_value, batch_size, pause_seconds)\n",
    "try_trans[try_trans['name_en'] == 'Error']\n",
    "\n",
    "name_mapping = try_trans.set_index('name_ru')['name_en'].to_dict()\n",
    "subclass_name_translated['name_en'] = subclass_name_translated.apply(\n",
    "    lambda row: name_mapping.get(row['name_ru'], row['name_en']) if row['name_en'] == 'Error' else row['name_en'], axis=1\n",
    ")\n",
    "\n",
    "# 975\n",
    "len(subclass_name_translated['name_en'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item type name\n",
    "item_type = pd.Series(catalog['item_type'].unique()).dropna()\n",
    "item_type_translated = batch_translate(item_type, batch_size, pause_seconds)\n",
    "\n",
    "# # ! Manual substitute ! Department\n",
    "# data = item_type_translated['name_en']\n",
    "# data[data.duplicated(keep=False)].unique()\n",
    "# ['Red', 'Green', 'Sausages', 'Coffee', 'Cottage cheese', 'Cakes',\n",
    "#        'White', 'Ham', 'Vegetables', 'Seeds', 'Rolls', 'Black',\n",
    "#        'Chocolate', 'Cream', 'Serum', 'Paste', 'Diapers', 'Jam',\n",
    "#        'National', 'Olive', 'Fresh', 'Crackers', 'Spick', 'Fruit',\n",
    "#        'Mashed potatoes', 'Mashed potatoes+side dish', 'Frozen',\n",
    "#        'Pancakes', 'Bulbs', 'Vegetable', 'Brushes', 'Egg', 'Smoked',\n",
    "#        'Blueberry', 'Radish', 'Red frozen', 'Corn', 'Cold']\n",
    "\n",
    "item_type_translated[item_type_translated['name_en'] == 'Cakes']\n",
    "item_type_translated.iloc[22].name_en = 'Cupcakes'\n",
    "\n",
    "item_type_translated[item_type_translated['name_en'] == 'Green']\n",
    "item_type_translated.iloc[3].name_en = 'Greens'\n",
    "\n",
    "item_type_translated[item_type_translated['name_en'] == 'Paste']\n",
    "item_type_translated.iloc[151].name_en = 'Pastille'\n",
    "item_type_translated.iloc[210].name_en = 'Pasta'\n",
    "# 674\n",
    "len(item_type_translated['name_en'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix error translation\n",
    "error_value = item_type_translated[item_type_translated['name_en'] == 'Error']['name_ru']\n",
    "# error_value\n",
    "try_trans = batch_translate(error_value, batch_size, pause_seconds)\n",
    "try_trans[try_trans['name_en'] == 'Error']\n",
    "\n",
    "name_mapping = try_trans.set_index('name_ru')['name_en'].to_dict()\n",
    "item_type_translated['name_en'] = item_type_translated.apply(\n",
    "    lambda row: name_mapping.get(row['name_ru'], row['name_en']) if row['name_en'] == 'Error' else row['name_en'], axis=1\n",
    ")\n",
    "\n",
    "# 635\n",
    "len(item_type_translated['name_en'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add English department name to catalog\n",
    "dept_catalog = pd.merge(catalog, dept_name_translated, left_on='dept_name', right_on='name_ru', how='left')\n",
    "dept_catalog.drop(columns=['name_ru'], inplace=True)\n",
    "dept_catalog.rename(columns={'name_en': 'dept_name_en'}, inplace=True)\n",
    "\n",
    "# Add Eng department name to catalog\n",
    "class_catalog = pd.merge(dept_catalog, class_name_translated, left_on='class_name', right_on='name_ru', how='left')\n",
    "class_catalog.drop(columns=['name_ru'], inplace=True)\n",
    "class_catalog.rename(columns={'name_en': 'class_name_en'}, inplace=True)\n",
    "\n",
    "# Add Eng subclass name to catalog\n",
    "subclass_catalog = pd.merge(class_catalog, subclass_name_translated, left_on='subclass_name', right_on='name_ru', how='left')\n",
    "subclass_catalog.drop(columns=['name_ru'], inplace=True)\n",
    "subclass_catalog.rename(columns={'name_en': 'subclass_name_en'}, inplace=True)\n",
    "\n",
    "final_catalog = pd.merge(subclass_catalog, item_type_translated, left_on='item_type', right_on='name_ru', how='left')\n",
    "final_catalog.drop(columns=['name_ru'], inplace=True)\n",
    "final_catalog.rename(columns={'name_en': 'item_type_en'}, inplace=True)\n",
    "\n",
    "dept_name_translated.to_csv('translate_products/dept_name_translated.csv')\n",
    "class_name_translated.to_csv('translate_products/class_name_translated.csv')\n",
    "subclass_name_translated.to_csv('translate_products/subclass_name_translated.csv')\n",
    "item_type_translated.to_csv('translate_products/item_type_translated.csv')\n",
    "\n",
    "final_catalog.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_catalog\n",
    "final_catalog.to_csv('final_catalog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete not used df from memory\n",
    "del dept_catalog\n",
    "del class_catalog\n",
    "del subclass_catalog\n",
    "del dept_name_translated\n",
    "del class_name_translated\n",
    "del subclass_name_translated\n",
    "del item_type_translated\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stores.csv\n",
    "Purpose: Contains stores info data.</br>\n",
    "Columns:</br>\n",
    "store_id: Store number</br>\n",
    "division: Store division</br>\n",
    "format: Store format</br>\n",
    "city: Location</br>\n",
    "area: Store sales area</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores  = pd.read_csv(root + 'stores.csv', index_col=0)\n",
    "\n",
    "# stores['name'] = stores['division'] + \" - \" + stores['format']\n",
    "# stores['location'] = stores['city'] + \" (\" + stores['area'].astype(str) + \" sqm)\"\n",
    "\n",
    "stores.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic clients, orders, managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Generate random clients\n",
    "def generate_clients(num_clients):\n",
    "    clients = []\n",
    "    for _ in range(num_clients):\n",
    "        dob = fake.date_of_birth(minimum_age=18, maximum_age=80)  # Generate date of birth between 18 and 80 years\n",
    "        age = datetime.now().year - dob.year\n",
    "        loyalty_card = str(random.randint(1000000000, 9999999999))  # 10-digit number\n",
    "        clients.append({\n",
    "            'client_id': fake.uuid4(),\n",
    "            'client_name': fake.first_name(),\n",
    "            'client_surname': fake.last_name(),\n",
    "            'client_email': fake.email(),\n",
    "            'client_phone': fake.phone_number(),\n",
    "            'client_dob': dob.strftime(\"%Y-%m-%d\"), \n",
    "            'client_age': age,\n",
    "            'client_loyalty_card': loyalty_card if random.random() > 0.17 else 0000000000  # 17% missing cards \n",
    "        })\n",
    "\n",
    "    return clients\n",
    "\n",
    "# Generate random managers\n",
    "def generate_managers(num_managers):\n",
    "    positions = ['Sales Manager', 'Store Manager', 'Regional Manager', 'Area Supervisor']\n",
    "    departments = ['Online Sales', 'Customer Service', 'Operations', 'Logistics']\n",
    "    \n",
    "    managers = []\n",
    "    for _ in range(num_managers):\n",
    "        managers.append({\n",
    "            'manager_id': fake.uuid4(),  # Unique manager ID\n",
    "            'manager_name': fake.first_name(),\n",
    "            'manager_surname': fake.last_name(),\n",
    "            'manager_position': random.choice(positions),  # Random position\n",
    "            'manager_department': random.choice(departments)  # Random department\n",
    "        })\n",
    "    return managers\n",
    "\n",
    "# Generate random orders\n",
    "def generate_orders(num_orders, managers_df):\n",
    "    orders = []\n",
    "    order_status_choices = ['Pending', 'Confirmed', 'Shipped', 'Delivered', 'Cancelled']\n",
    "    payment_methods = ['Credit Card', 'PayPal', 'Bank Transfer', 'Cash on Delivery']\n",
    "    currencies = ['USD', 'EUR']\n",
    "    packaging_choices = ['Standard', 'Gift Wrap', 'Eco-friendly', 'Custom Packaging']\n",
    "    for _ in range(num_orders):\n",
    "        manager = managers_df.sample(1).iloc[0] # random manager\n",
    "        orders.append({\n",
    "            'order_id': fake.uuid4(),  \n",
    "            'order_number': fake.ean(length=8), \n",
    "            'order_payment_type': random.choice(payment_methods),  \n",
    "            'order_status': random.choice(order_status_choices), \n",
    "            'order_currency': random.choice(currencies), \n",
    "            'order_packaging_instructions': random.choice(packaging_choices),\n",
    "            'manager_id': manager['manager_id'] \n",
    "        })\n",
    "            \n",
    "    return orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = generate_clients(100_000) # 100 000 clients for 2 years\n",
    "\n",
    "# Loading the list to a file and reading it into a DataFrame works significantly faster (18.7 seconds) \n",
    "# than transforming the list directly into a DataFrame (over 22 minutes).\n",
    "json_result = json.dumps(clients, indent=4)\n",
    "with open(root + \"clients.json\", \"w\") as json_file:\n",
    "    json_file.write(json_result)\n",
    "\n",
    "print('json write')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cliends df and add Nan values\n",
    "clients_df = pd.read_json(root + \"clients.json\")\n",
    "for col in ['client_phone', 'client_email', 'client_dob', 'client_age']:\n",
    "    clients_df.loc[clients_df.sample(frac=0.3).index, col] = np.nan  # 30% of rows\n",
    "\n",
    "clients_df.to_csv(root + \"clients.csv\")\n",
    "\n",
    "clients_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "managers= generate_managers(100) # 100 manager for 2 years\n",
    "managers_df = pd.DataFrame(managers)\n",
    "managers_df.to_csv(root + 'managers.csv')\n",
    "managers_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = generate_orders(250_000, managers_df) # 250 000 orders in 2 years\n",
    "json_result = json.dumps(orders, indent=4)\n",
    "with open(root + \"orders.json\", \"w\") as json_file:\n",
    "     json_file.write(json_result)\n",
    "\n",
    "print('json write')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create orders df and add Nan values\n",
    "orders_df = pd.read_json(root + \"orders.json\")\n",
    "for col in ['order_payment_type', 'order_status', 'order_currency', 'order_packaging_instructions']:\n",
    "    orders_df.loc[orders_df.sample(frac=0.3).index, col] = np.nan  # 30%\n",
    "\n",
    "orders_df.to_csv(root + \"orders.csv\")\n",
    "\n",
    "orders_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sales.csv and online.csv\n",
    "Purpose: 1 This file contains aggregated store sales for specific dates. </br>\n",
    "Purpose: 2 This file contains aggregated online sales by store for specific dates.</br>\n",
    "Columns:</br>\n",
    "date: Sales date</br>\n",
    "item_id: A unique identifier for each product</br>\n",
    "quantity: Total quantity of product sold per day</br>\n",
    "price_base: Average sales price per day</br>\n",
    "sum_total: Total daily sales amount</br>\n",
    "store_id: Store number</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 7) (100000, 8) (100, 5)\n"
     ]
    }
   ],
   "source": [
    "# Create online sales file\n",
    "online_sales  = pd.read_csv(root + 'online.csv', index_col=0)\n",
    "stores  = pd.read_csv(root + 'stores.csv', index_col=0)\n",
    "\n",
    "price_history  = pd.read_csv(root + 'price_history.csv', index_col=0)\n",
    "\n",
    "catalog = pd.read_csv('final_catalog.csv', index_col=0)\n",
    "catalog = catalog.drop(columns=['dept_name', 'class_name' , 'subclass_name', 'item_type'])\n",
    "catalog.columns\n",
    "\n",
    "orders_df = pd.read_csv(root + 'orders.csv', index_col=0)\n",
    "clients_df = pd.read_csv(root + 'clients.csv', index_col=0)\n",
    "managers_df = pd.read_csv(root + 'managers.csv', index_col=0)\n",
    "\n",
    "print(orders_df.shape, clients_df.shape, managers_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine everything\n",
    "\n",
    "orders_df = orders_df.merge(managers_df, on='manager_id', how='left')  # Order + manager\n",
    "online_sales['order_id'] = random.choices(orders_df['order_id'], k=len(online_sales))  # + new random column\n",
    "orders_df['client_id'] = random.choices(clients_df['client_id'], k=len(orders_df))  # + new random column\n",
    "online_sales = online_sales.merge(orders_df, on='order_id', how='left')  # sales + orders\n",
    "online_sales = online_sales.merge(clients_df, on='client_id', how='left')  # sales + client\n",
    "online_sales = online_sales.merge(catalog, on='item_id', how='left')  # sales + products\n",
    "online_sales = online_sales.merge(stores, on='store_id', how='left')  # sales + store\n",
    "\n",
    "online_sales.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculste sales\n",
    "online_sales['sum_total'] = online_sales['price_base'] * online_sales['quantity']\n",
    "\n",
    "# Create cost value\n",
    "online_sales = pd.merge(\n",
    "    online_sales, \n",
    "    price_history[['date', 'item_id', 'price', 'store_id']],\n",
    "    on=['date', 'item_id', 'store_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "online_sales.rename(columns={'price': 'price_history'}, inplace=True)\n",
    "\n",
    "max_price_per_item = price_history.groupby('item_id')['price'].max().reset_index()\n",
    "max_price_per_item.rename(columns={'price': 'price_max'}, inplace=True)\n",
    "\n",
    "online_sales = pd.merge(\n",
    "    online_sales, \n",
    "    max_price_per_item,\n",
    "    on=['item_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "online_sales.insert(5, 'cost', online_sales['price_history'].fillna(online_sales['price_max']))\n",
    "condition = 100 - (online_sales['price_base'] * 100 / online_sales['cost']) > 99\n",
    "online_sales.loc[condition, 'cost'] = np.nan\n",
    "\n",
    "online_sales['cost'] = online_sales['cost'].fillna(online_sales['price_base'])\n",
    "\n",
    "# Drop the 'price' column if not needed\n",
    "online_sales = online_sales.drop(columns=['price_history'])\n",
    "online_sales = online_sales.drop(columns=['price_max'])\n",
    "\n",
    "online_sales.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_sales.to_csv('online_sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_sales.loc[:100].to_csv('online_sales_sample_100_rows.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline sales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zd/rvz5yw3x6szdz0mdyst0tdz00000gn/T/ipykernel_13956/1479779569.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  reduced_sales = grouped.apply(lambda x: x.sample(frac=sampling_fraction, random_state=42)).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1500018, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crop dataset from 7.1 million records to 1.5 million records\n",
    "sales  = pd.read_csv(root + 'sales.csv', index_col=0)\n",
    "\n",
    "sales['date'] = pd.to_datetime(sales['date'])\n",
    "\n",
    "# Group by date to ensure proportional sampling across all dates\n",
    "grouped = sales.groupby(sales['date'])\n",
    "\n",
    "# Target total number of records\n",
    "target_records = 1_500_000\n",
    "\n",
    "# Calculate the sampling fraction\n",
    "sampling_fraction = target_records / len(sales)\n",
    "\n",
    "# Sample from each group proportionally\n",
    "reduced_sales = grouped.apply(lambda x: x.sample(frac=sampling_fraction, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "del sales\n",
    "\n",
    "reduced_sales.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500018, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offline_sales = reduced_sales.merge(catalog, on='item_id', how='left')  # sales + products\n",
    "offline_sales = offline_sales.merge(stores, on='store_id', how='left')  # sales + store\n",
    "\n",
    "offline_sales['client_id'] = random.choices(clients_df['client_id'], k=len(offline_sales))  # + new random column\n",
    "num_nan = int(len(offline_sales) * 0.7)  # Calculate the number of NaN rows, 70%\n",
    "# Randomly select rows to assign NaN to client_id\n",
    "nan_indices = random.sample(range(len(offline_sales)), num_nan)\n",
    "offline_sales.loc[nan_indices, 'client_id'] = np.nan\n",
    "\n",
    "offline_sales = offline_sales.merge(clients_df, on='client_id', how='left')  # sales + client\n",
    "\n",
    "# recalculste sales\n",
    "offline_sales['sum_total'] = offline_sales['price_base'] * offline_sales['quantity']\n",
    "\n",
    "offline_sales.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_sales.to_csv('offline_sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1506299, 26)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offline_sales  = pd.read_csv('offline_sales.csv', index_col=0)\n",
    "\n",
    "# Create cost value\n",
    "offline_sales = pd.merge(\n",
    "    offline_sales, \n",
    "    price_history[['date', 'item_id', 'price', 'store_id']],\n",
    "    on=['date', 'item_id', 'store_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "offline_sales.rename(columns={'price': 'price_history'}, inplace=True)\n",
    "\n",
    "max_price_per_item = price_history.groupby('item_id')['price'].max().reset_index()\n",
    "max_price_per_item.rename(columns={'price': 'price_max'}, inplace=True)\n",
    "\n",
    "offline_sales = pd.merge(\n",
    "    offline_sales, \n",
    "    max_price_per_item,\n",
    "    on=['item_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "offline_sales.insert(5, 'cost', offline_sales['price_history'].fillna(offline_sales['price_max']))\n",
    "condition = 100 - (offline_sales['price_base'] * 100 / offline_sales['cost']) > 99\n",
    "offline_sales.loc[condition, 'cost'] = np.nan\n",
    "\n",
    "offline_sales['cost'] = offline_sales['cost'].fillna(offline_sales['price_base'])\n",
    "\n",
    "# Drop the 'price' column if not needed\n",
    "offline_sales = offline_sales.drop(columns=['price_history'])\n",
    "offline_sales = offline_sales.drop(columns=['price_max'])\n",
    "\n",
    "offline_sales.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>item_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price_base</th>\n",
       "      <th>sum_total</th>\n",
       "      <th>cost</th>\n",
       "      <th>store_id</th>\n",
       "      <th>weight_volume</th>\n",
       "      <th>weight_netto</th>\n",
       "      <th>fatness</th>\n",
       "      <th>...</th>\n",
       "      <th>city</th>\n",
       "      <th>area</th>\n",
       "      <th>client_id</th>\n",
       "      <th>client_name</th>\n",
       "      <th>client_surname</th>\n",
       "      <th>client_email</th>\n",
       "      <th>client_phone</th>\n",
       "      <th>client_dob</th>\n",
       "      <th>client_age</th>\n",
       "      <th>client_loyalty_card</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-28</td>\n",
       "      <td>d3deb022e0fa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.9</td>\n",
       "      <td>42.9</td>\n",
       "      <td>49.9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>City1</td>\n",
       "      <td>210</td>\n",
       "      <td>2d5c1246-5290-4197-bed2-2d23a3f0b455</td>\n",
       "      <td>Amanda</td>\n",
       "      <td>Cross</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8039922913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-28</td>\n",
       "      <td>94d6933ef685</td>\n",
       "      <td>3.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>City1</td>\n",
       "      <td>210</td>\n",
       "      <td>c08a5a9e-6a35-454e-829e-46ac5512376b</td>\n",
       "      <td>Sara</td>\n",
       "      <td>Miles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+1-355-261-3860x6386</td>\n",
       "      <td>1976-06-03</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-28</td>\n",
       "      <td>e31cabac26f5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>259.9</td>\n",
       "      <td>1819.3</td>\n",
       "      <td>499.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>City1</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date       item_id  quantity  price_base  sum_total   cost  store_id  \\\n",
       "0  2022-08-28  d3deb022e0fa       1.0        42.9       42.9   49.9         2   \n",
       "1  2022-08-28  94d6933ef685       3.0       200.0      600.0  240.0         2   \n",
       "2  2022-08-28  e31cabac26f5       7.0       259.9     1819.3  499.9         1   \n",
       "\n",
       "   weight_volume  weight_netto  fatness  ...   city  area  \\\n",
       "0          0.045         0.045      NaN  ...  City1   210   \n",
       "1            NaN           NaN      NaN  ...  City1   210   \n",
       "2          0.200         0.200      NaN  ...  City1  1500   \n",
       "\n",
       "                              client_id client_name client_surname  \\\n",
       "0  2d5c1246-5290-4197-bed2-2d23a3f0b455      Amanda          Cross   \n",
       "1  c08a5a9e-6a35-454e-829e-46ac5512376b        Sara          Miles   \n",
       "2                                   NaN         NaN            NaN   \n",
       "\n",
       "  client_email          client_phone  client_dob client_age  \\\n",
       "0          NaN            8039922913         NaN       45.0   \n",
       "1          NaN  +1-355-261-3860x6386  1976-06-03       49.0   \n",
       "2          NaN                   NaN         NaN        NaN   \n",
       "\n",
       "  client_loyalty_card  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 NaN  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offline_sales.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_sales.to_csv('offline_sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_sales.loc[:100].to_csv('offline_sales_sample_100_rows.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_history  = pd.read_csv(root + 'price_history.csv', index_col=0)\n",
    "actual_matrix  = pd.read_csv(root + 'actual_matrix.csv', index_col=0)\n",
    "discounts_history  = pd.read_csv(root + 'discounts_history.csv', index_col=0)\n",
    "markdowns  = pd.read_csv(root + 'markdowns.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## markdowns.csv\n",
    "Purpose: This file provides data on products sold at markdown prices in each store. </br>\n",
    "Columns:</br>\n",
    "date: Date of markdown</br>\n",
    "item_id: A unique identifier for each product</br>\n",
    "normal_price: Regular price</br>\n",
    "price: Price during markdown</br>\n",
    "quantity: Quantity sold at markdown</br>\n",
    "store_id: Store number</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdowns.head(3) # уцінка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## price_history.csv\n",
    "Purpose: This file contains price changes data in each store.</br>\n",
    "Columns:</br>\n",
    "date: Date of price change</br>\n",
    "item_id: A unique identifier for each product</br>\n",
    "price: Item new price</br>\n",
    "code: Price change code</br>\n",
    "store_id: Store number</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_history.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2023-08-04'\t\n",
    "item_id = '78a8896d4474' \t\n",
    "# 3.0\t12.40\t37.21\t1-store\n",
    "\n",
    "price_history[price_history['item_id'] == item_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## actual_matrix.csv\n",
    "Purpose: Contains the list of products available in stores.</br>\n",
    "Columns:</br>\n",
    "item_id: A unique identifier for each product</br>\n",
    "date: Date of last product appearance in the current matrix</br>\n",
    "store_id: Store number</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_matrix.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discounts_history.csv\n",
    "Purpose: Contains historical promo data for each specific store.</br>\n",
    "Columns:</br>\n",
    "date: Date</br>\n",
    "item_id: A unique identifier for each product</br>\n",
    "sale_price_before_promo: Price before promo period started</br>\n",
    "sale_price_time_promo: Price during the promo period</br>\n",
    "promo_type_code: Promo code type</br>\n",
    "doc_id: Promo document number</br>\n",
    "number_disc_day: Sequential day number of the current promo period</br>\n",
    "store_id: Store number</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discounts_history.head(3)\n",
    "\n",
    "discounts_history[(discounts_history['item_id'] == item_id)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cost value\n",
    "merged_df = pd.merge(\n",
    "    online_sales, \n",
    "    price_history[['date', 'item_id', 'price', 'store_id']],\n",
    "    on=['date', 'item_id', 'store_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "merged_df.rename(columns={'price': 'price_history'}, inplace=True)\n",
    "\n",
    "max_price_per_item = price_history.groupby('item_id')['price'].max().reset_index()\n",
    "max_price_per_item.rename(columns={'price': 'price_max'}, inplace=True)\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    merged_df, \n",
    "    max_price_per_item,\n",
    "    on=['item_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "merged_df['cost'] = merged_df['price_history'].fillna(merged_df['price_max'])\n",
    "condition = 100 - (merged_df['price_base'] * 100 / merged_df['cost']) > 99\n",
    "merged_df.loc[condition, 'cost'] = np.nan\n",
    "\n",
    "merged_df['cost'] = merged_df['cost'].fillna(merged_df['price_base'])\n",
    "\n",
    "# Drop the 'price' column if not needed\n",
    "merged_df = merged_df.drop(columns=['price_history'])\n",
    "merged_df = merged_df.drop(columns=['price_max'])\n",
    "\n",
    "merged_df[['item_id', 'quantity', 'sum_total', 'price_base', 'cost']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[(100 - (merged_df['price_base'] * 100 / merged_df['cost']) > 99) == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_sales  = pd.read_csv('offline_sales.csv', index_col=0)\n",
    "online_sales  = pd.read_csv('online_sales.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'item_id', 'quantity', 'price_base', 'sum_total', 'cost',\n",
       "       'store_id', 'weight_volume', 'weight_netto', 'fatness', 'dept_name_en',\n",
       "       'class_name_en', 'subclass_name_en', 'item_type_en', 'division',\n",
       "       'format', 'city', 'area', 'client_id', 'client_name', 'client_surname',\n",
       "       'client_email', 'client_phone', 'client_dob', 'client_age',\n",
       "       'client_loyalty_card'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offline_sales.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'item_id', 'quantity', 'price_base', 'sum_total', 'cost',\n",
       "       'store_id', 'order_id', 'order_number', 'order_payment_type',\n",
       "       'order_status', 'order_currency', 'order_packaging_instructions',\n",
       "       'manager_id', 'manager_name', 'manager_surname', 'manager_position',\n",
       "       'manager_department', 'client_id', 'client_name', 'client_surname',\n",
       "       'client_email', 'client_phone', 'client_dob', 'client_age',\n",
       "       'client_loyalty_card', 'weight_volume', 'weight_netto', 'fatness',\n",
       "       'dept_name_en', 'class_name_en', 'subclass_name_en', 'item_type_en',\n",
       "       'division', 'format', 'city', 'area'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_sales.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>client_name</th>\n",
       "      <th>client_surname</th>\n",
       "      <th>client_email</th>\n",
       "      <th>client_phone</th>\n",
       "      <th>client_dob</th>\n",
       "      <th>client_age</th>\n",
       "      <th>client_loyalty_card</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c61ebd17-27af-45ac-8c50-29df6e870abf</td>\n",
       "      <td>Karen</td>\n",
       "      <td>Velazquez</td>\n",
       "      <td>mevans@example.com</td>\n",
       "      <td>843.744.3692x87377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1680045020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7d892099-2477-401a-b3f6-2804cb14b194</td>\n",
       "      <td>Tara</td>\n",
       "      <td>Smith</td>\n",
       "      <td>leeleslie@example.org</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9922529282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42a7384e-b915-432f-b65b-070eb5461e55</td>\n",
       "      <td>Matthew</td>\n",
       "      <td>Macias</td>\n",
       "      <td>dianasmith@example.net</td>\n",
       "      <td>7462250130</td>\n",
       "      <td>1985-07-28</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6859107055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              client_id client_name client_surname  \\\n",
       "0  c61ebd17-27af-45ac-8c50-29df6e870abf       Karen      Velazquez   \n",
       "1  7d892099-2477-401a-b3f6-2804cb14b194        Tara          Smith   \n",
       "2  42a7384e-b915-432f-b65b-070eb5461e55     Matthew         Macias   \n",
       "\n",
       "             client_email        client_phone  client_dob  client_age  \\\n",
       "0      mevans@example.com  843.744.3692x87377         NaN         NaN   \n",
       "1   leeleslie@example.org                 NaN         NaN         NaN   \n",
       "2  dianasmith@example.net          7462250130  1985-07-28        40.0   \n",
       "\n",
       "   client_loyalty_card  \n",
       "0           1680045020  \n",
       "1           9922529282  \n",
       "2           6859107055  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_sales[['client_id', 'client_name', 'client_surname',\n",
    "       'client_email', 'client_phone', 'client_dob', 'client_age',\n",
    "       'client_loyalty_card']].head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
